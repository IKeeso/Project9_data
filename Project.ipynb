{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVc9jnAag9RWNbtfN/IEW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IKeeso/Project9_data/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY5TNdTtkz2c"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "!pip install conllu\n",
        "!git clone https://github.com/IKeeso/Project9_data.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "from stanza.utils.conll import CoNLL\n",
        "from conllu import parse\n",
        "from conllu import TokenList\n",
        "import re\n",
        "\n",
        "!ls Project9_data\n"
      ],
      "metadata": {
        "id": "yCcYfhBIiiLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download('sv')\n",
        "nlp = stanza.Pipeline(lang='sv')"
      ],
      "metadata": {
        "id": "O8ZSbpC373Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_and_convert(filename):\n",
        "  %cd /content/Project9_data\n",
        "  try:\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "      text = file.read()\n",
        "      file.close()\n",
        "  except:\n",
        "    text = filename\n",
        "  doc = nlp(text)\n",
        "  pathname = filename.split(\".\")[0]\n",
        "  conllufilename = f\"{pathname}.conllu\"\n",
        "  CoNLL.write_doc2conll(doc, conllufilename)"
      ],
      "metadata": {
        "id": "dgejjVHjCtAL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_and_convert(\"expressen.txt\")\n",
        "tag_and_convert(\"Personer som är på spektrumet. Personer på spektrumet. Personer som har autism\")"
      ],
      "metadata": {
        "id": "9qqMAKmDTjJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticAnalyzer:\n",
        "  def __init__(self, file_path):\n",
        "    self.file_path = file_path\n",
        "    self.data = self.parse_conll_u(self.file_path)\n",
        "    self.PFL_sentence_matches = self.person_first_lang(self.data)\n",
        "    self.IFL_sentence_matches = self.identity_first_lang(self.data)\n",
        "\n",
        "  def parse_conll_u(self, file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      data = {}\n",
        "      sentence = []\n",
        "      for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith('# text'):\n",
        "          sentence = []\n",
        "        if line.startswith(\"# sent_id\"):\n",
        "          line = line.split(\"=\")\n",
        "          key = line[1].strip()\n",
        "          sentence = []\n",
        "        elif line == '':\n",
        "          if sentence:\n",
        "            data[key] = sentence\n",
        "        else:\n",
        "            sentence.append(line.split('\\t'))\n",
        "    return data\n",
        "\n",
        "  def person_first_lang(self, data):\n",
        "    PFL_sents = {}\n",
        "    PFL_regx = re.compile(r\"har|med\", re.IGNORECASE)\n",
        "\n",
        "    for id, sentence in data.items():\n",
        "      if self.has_autism(data[id]):\n",
        "        for token in sentence:\n",
        "          if token[1] == \"autism\":\n",
        "            if sentence[int(token[0])-2][1] == \"och\".lower():\n",
        "              if PFL_regx.match(sentence[int(sentence[int(token[6])-1][int(6)])-1][1]):\n",
        "                PFL_sents[id] = sentence\n",
        "            if PFL_regx.match(sentence[int(token[0])-2][1]):\n",
        "              PFL_sents[id] = sentence\n",
        "    return PFL_sents\n",
        "\n",
        "  def identity_first_lang(self, data):\n",
        "    IFL_sents = {}\n",
        "    IFL_regx = re.compile(r\"^.*(autistisk|autistiska)\", re.IGNORECASE)\n",
        "\n",
        "    for id, sentence in data.items():\n",
        "      words = ' '.join(token[1] for token in sentence)\n",
        "      if IFL_regx.match(words):\n",
        "        IFL_sents[id] = sentence\n",
        "\n",
        "    return IFL_sents\n",
        "\n",
        "\n",
        "  def has_autism(self, sentence):\n",
        "    autism_regx = re.compile(r\"^.*\\bautism|spektrumet\")\n",
        "    words = ' '.join(token[1] for token in sentence)\n",
        "    if autism_regx.match(words):\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "####\n",
        "\n",
        "def main(file, lang):\n",
        "  analyzer = SemanticAnalyzer(file_path=f\"{file.lower()}\")\n",
        "  if lang.lower() == \"pfl\":\n",
        "    matches = analyzer.PFL_sentence_matches\n",
        "  elif lang.lower() == \"ifl\":\n",
        "    matches = analyzer.IFL_sentence_matches\n",
        "  else:\n",
        "    print(\"Invalid language choice. Please choose either 'PFL' or 'IFL'.\")\n",
        "    return\n",
        "\n",
        "  for id, sentence in matches.items():\n",
        "    print(f\"Sentence ID: {id} | {' '.join(token[1] for token in sentence)}\")\n",
        "    print(\"\")\n",
        "\n",
        "  print(f\"Total matching sentences: {len(matches)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = input(\"Enter the file path: \")\n",
        "    if file_path.split(\".\")[1] != \"conllu\":\n",
        "      tag_and_convert(file_path)\n",
        "      file_path = f\"{file_path.split('.')[0]}.conllu\"\n",
        "    lang = input(\"Enter the language (PFL or IFL): \")\n",
        "    main(file_path, lang)"
      ],
      "metadata": {
        "id": "tCyasQHAyXHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1280bf5-5d81-493a-f5d0-f787bfd86e9c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file path: expressen.txt\n",
            "/content/Project9_data\n",
            "Enter the language (PFL or IFL): ifl\n",
            "Sentence ID: 0 | ” Det ser otroligt olika ut för olika autistiska personer ”\n",
            "\n",
            "Sentence ID: 9 | – Det ser otroligt olika ut för olika autistiska personer och spektrumet kan vara stort i en och samma person , säger hon .\n",
            "\n",
            "Sentence ID: 14 | – Stödet från Postkodlotteriet gör att vi kan erbjuda läger för autistiska barn och unga med deras föräldrar .\n",
            "\n",
            "Total matching sentences: 3\n"
          ]
        }
      ]
    }
  ]
}